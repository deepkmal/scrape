[
  {
    "score": 6938,
    "title": "data.cdc.gov full archive",
    "selftext": "Good morning r/DataHoarder,\n\nMany of you have probably seen me working on the CDC datasets archive, but those thread have gotten a bit cluttered and I have a lot of people to notify, so I'm making this a new post.\n\nOver the past several days I've been archiving and uploading a copy of all public datasets formerly available at [data.cdc.gov](http://data.cdc.gov), as of 2025-01-28. This does not include webpages themselves, as those have already largely been archived by projects like EOTArchive and the Wayback Machine.\n\nThis upload is now complete and available at https://archive.org/details/20250128-cdc-datasets. For seeders use the file \"full-20250128-cdc-datasets-USETHIS.torrent\" included in the files or the magnet at the end of this post.\n\nFor more context have a look at [this post](https://www.reddit.com/r/DataHoarder/comments/1ibnjbb/altcdc_bluesky_account_warns_of_impending_data/) and [this post](https://www.reddit.com/r/DataHoarder/comments/1iekywr/cdc_website_going_down_by_eod/).\n\nThank you to everyone who requested this important data, and particularly to those who have offered to mirror it. I'll ping everyone who has requested notice ~~in a comment~~, unless you DMed me requesting notice in which case I'll respond to your message.\n\nHappy hoarding everyone!\n\n  \nBrief ETA: Reddit is really not a fan of bulk pinging apparently, so I'll have to go back through the thread to notify everyone. That'll take some time, so apologies for that.\n\n\n\nTorrent mirror:\n\nmagnet:?xt=urn:btih:3bf9d780d838b6bbc977e9cc6a9530e70ec49732&amp;dn=20250128-cdc-datasets&amp;tr=udp%3A%2F%2Ftracker.0x7c0.com%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Fexodus.desync.com%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Fexplodie.org%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Fopen.free-tracker.ga%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.qu.ax%3A6969%2Fannounce&amp;tr=http%3A%2F%2Fopen.tracker.cl%3A1337%2Fannounce&amp;tr=udp%3A%2F%2Fns-1.x-fins.com%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.bittor.pw%3A1337%2Fannounce&amp;tr=udp%3A%2F%2Ftracker-udp.gbitt.info%3A80%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.ololosh.space%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Fopen.demonii.com%3A1337%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.tiny-vps.com%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Fopen.stealth.si%3A80%2Fannounce&amp;tr=udp%3A%2F%2Fopen.dstud.io%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.dler.org%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Fopentracker.io%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.dump.cl%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.theoks.net%3A6969%2Fannounce&amp;tr=udp%3A%2F%2Ftracker.torrent.eu.org%3A451%2Fannounce",
    "subreddit": "DataHoarder",
    "upvote_ratio": 0.98,
    "subreddit_type": "public",
    "ups": 6938,
    "downs": 0,
    "created_utc": 1738438364.0,
    "media": null,
    "is_video": false,
    "num_comments": 5,
    "num_reports": null,
    "over_18": false,
    "category": "other",
    "category_confidence": 0.55,
    "category_rationale": "Announcement/informational post about archiving datasets without a clear step-by-step or model.",
    "composite_score": 4.965372512682104
  },
  {
    "score": 5111,
    "title": "Free: Thousands of tapes preserved. 2004~2009 CNN/MSNBC/FOX News recorded at home in Ann Arbor area",
    "selftext": "SOLVED: THESE TAPES HAVE BEEN DONATED TO THE INTERNET ARCHIVE. Thank you EVERYONE for your inquiry's and interest in the tapes.\nAbout 18 boxes have been taken so far. Wanting to give them to someone who is going to save and digitize the tapes. I think the commercials might be even more valuable than the news, but there is Hurricaine Katrina Coverage here too. They're in McDonalds food boxes because the woman who recorded these worked at McDonald's at one time. ",
    "subreddit": "DataHoarder",
    "upvote_ratio": 0.98,
    "subreddit_type": "public",
    "ups": 5111,
    "downs": 0,
    "created_utc": 1762905268.0,
    "media": null,
    "is_video": false,
    "num_comments": 281,
    "num_reports": null,
    "over_18": false,
    "category": "other",
    "category_confidence": 0.6,
    "category_rationale": "Announcement of donated tapes and preservation effort; informational rather than instructional.",
    "composite_score": 5.668715399310025
  },
  {
    "score": 5014,
    "title": "Harvard's Library Innovation Lab just released all 311,000 datasets from data.gov, totalling 16 TB",
    "selftext": "The blog post is here: [https://lil.law.harvard.edu/blog/2025/02/06/announcing-data-gov-archive/](https://lil.law.harvard.edu/blog/2025/02/06/announcing-data-gov-archive/)\n\nHere's the full text:\n\n&gt;**Announcing the** **Data.gov** **Archive**\n\n&gt;Today we released our [archive of data.gov](https://source.coop/repositories/harvard-lil/gov-data/description) on Source Cooperative. The 16TB collection includes over 311,000 datasets harvested during 2024 and 2025, a complete archive of federal public datasets linked by data.gov. It will be updated daily as new datasets are added to data.gov.\n\n&gt;This is the first release in our new [data vault project](https://lil.law.harvard.edu/blog/2025/01/30/preserving-public-u-s-federal-data/) to preserve and authenticate vital public datasets for academic research, policymaking, and public use.\n\n&gt;We’ve built this project on our long-standing commitment to preserving government records and making public information available to everyone. Libraries play an essential role in safeguarding the integrity of digital information. By preserving detailed metadata and establishing digital signatures for authenticity and provenance, we make it easier for researchers and the public to cite and access the information they need over time.\n\n&gt;In addition to the data collection, we are releasing [open source software and documentation](https://github.com/harvard-lil/data-vault) for replicating our work and creating similar repositories. With these tools, we aim not only to preserve knowledge ourselves but also to empower others to save and access the data that matters to them.\n\n&gt;For suggestions and collaboration on future releases, please contact us at [lil@law.harvard.edu](mailto:lil@law.harvard.edu).\n\n&gt;This project builds on our work with the [Perma.cc](https://perma.cc/) web archiving tool used by courts, law journals, and law firms; the [Caselaw Access Project](https://case.law/), sharing all precedential cases of the United States; and our research on [Century Scale Storage](https://lil.law.harvard.edu/century-scale-storage/). This work is made possible with support from the Filecoin Foundation for the Decentralized Web and the Rockefeller Brothers Fund.\n\nYou can follow the Library Innovation on Bluesky [here](https://bsky.app/profile/did:plc:xwf6xvqychkupgnumbyoru5l).\n\n___\n\n***Edit (2025-02-07 at 01:30 UTC):***\n\nu/lyndamkellam, a university data librarian, makes an important caveat [here](https://www.reddit.com/r/DataHoarder/comments/1ijhybf/comment/mbea3b4/).",
    "subreddit": "DataHoarder",
    "upvote_ratio": 0.99,
    "subreddit_type": "public",
    "ups": 5014,
    "downs": 0,
    "created_utc": 1738887691.0,
    "media": null,
    "is_video": false,
    "num_comments": 69,
    "num_reports": null,
    "over_18": false,
    "category": "value-add",
    "category_confidence": 0.55,
    "category_rationale": "Discusses a data archival project and its aims, offering context on preservation.",
    "composite_score": 5.365319957363566
  },
  {
    "score": 4250,
    "title": "We aren't hoarders, we are a line of defence that people didn't know they needed.",
    "selftext": "During the onset of the Russian/Ukrainian war we mobilised to archive as much information and data about it, from Ukranian gov sites to small blogs that would be lost forever. We worked with OSINT communities to ensure easy and quick access to any data needed. During the Twitter buyout, we did the same thing... and once again it falls on us to archive, hoard, preserve and share data that would be \"lost\".\n\nWe are a line of defence for freedom, not the only one, not a big one, but still an important one.",
    "subreddit": "DataHoarder",
    "upvote_ratio": 0.95,
    "subreddit_type": "public",
    "ups": 4250,
    "downs": 0,
    "created_utc": 1738560604.0,
    "media": null,
    "is_video": false,
    "num_comments": 155,
    "num_reports": null,
    "over_18": false,
    "category": "rant",
    "category_confidence": 0.45,
    "category_rationale": "Assertive moral framing of archivists as defenders, with emotional emphasis.",
    "composite_score": 5.437553404144353
  },
  {
    "score": 4188,
    "title": "The University wanted me to pay 700$ for a dataset, so I recreated it myself",
    "selftext": "Between the 1968 and 1976 the United States Department of Education, Office for Civil Rights conducted a School Desegregation Survey. I wanted to access it for my latest video, but when I wanted to download it ICPSR databse, i found that I needed to write a request and pay administrative fee of 700 dollars.\n\nSo I found that at the Library of Congress a binary version of these files are stored, encoded using EBCDIC. Using the scanned technical documentation for the survey, after around 2 days of trial and error, I managed to write a Python script to extract all this to .csv, and I'm releasing it publicly for free:  \n[https://github.com/borysthe/Elementary-and-Secondary-School-Civil-Rights-Survey-Results](https://github.com/borysthe/Elementary-and-Secondary-School-Civil-Rights-Survey-Results)",
    "subreddit": "DataHoarder",
    "upvote_ratio": 0.98,
    "subreddit_type": "public",
    "ups": 4188,
    "downs": 0,
    "created_utc": 1760181929.0,
    "media": null,
    "is_video": false,
    "num_comments": 86,
    "num_reports": null,
    "over_18": false,
    "category": "value-add",
    "category_confidence": 0.5,
    "category_rationale": "Narrative of recreating a dataset to improve access, with an example and link.",
    "composite_score": 5.32686998667053
  },
  {
    "score": 3969,
    "title": "API Clusterfuck! ~ We're locked, read this.",
    "selftext": "**[See reopening post.....](https://old.reddit.com/r/DataHoarder/comments/14j2kty/api_clusterfuck_reddit_said_fuck_you_we_dont_care/)** \n----------\n\n\n---\n\nHi everyone, we'll keep this short, you already know what's going on.\n\nAs you've almost certainly heard by now Reddit is locking down their API starting July 1st with the introduction of paid usage. These changes are what killed pushshift.io (full reddit archives and searchable api used by mods and many research/academic papers) and what will kill most (if not all) third-party reddit clients. This is obviously a detriment to everyone, and while Reddit will almost certainly go through with these changes regardless, thousands of subreddits are going to be participating in a 2-day (or longer) blackout. You can read more about the blackouts at r/ModCoord. At the very least, the planned blackout seems to have convinced Reddit to give free API access to accessibility clients. Hopefully it can change their minds further.\n\nr/DataHoarder will be locked for an undetermined amount of time, see [this thread](https://old.reddit.com/r/DataHoarder/comments/1479c7b/historic_reddit_archives_ongoing_archival_effort) for reddit data archives, tools, etc. we will also be using this time to update our sidebar links and do some general maintenance in the hopes that this mess doesn't mean the end for us and the many communities that see this as a killing of the Reddit we have loved over the years. \n\n**Note; during this time no new posts can be made and all comments are black-holed.** \n--------\n\n~ The Mod Team, ciao for now.\n\n--------------\n\nTrack the blackout here: https://reddark.untone.uk",
    "subreddit": "DataHoarder",
    "upvote_ratio": 0.94,
    "subreddit_type": "public",
    "ups": 3969,
    "downs": 0,
    "created_utc": 1686528261.0,
    "media": null,
    "is_video": false,
    "num_comments": 4,
    "num_reports": null,
    "over_18": false,
    "category": "rant",
    "category_confidence": 0.8,
    "category_rationale": "Expresses frustration about API changes and platform policies.",
    "composite_score": 4.653275508931125
  },
  {
    "score": 3841,
    "title": "NSFW subreddit purge, many subs have been banned today.",
    "selftext": "There's been a massive purge of many NSFW or Drug related subreddits today.   \nThis post is for any subreddit purge related discussion, other posts will be removed.   \n\nThis is a good reminder that nothing is permanent, and that anything that isnt stored within your own control can easily be removed.   \nKeeping your own backups/archives is a good way to preserve the things you want to keep.\n\nEdit:  \nSupposedly this was a \"bug\", reddit admin comment here: - /r/ModSupport/comments/1ii67mt/communities_are_banned_again_for_being_unmoderated/mb3fewv/    \nSeveral subs are still banned though.\n\nEdit 2:  \nThis was aparently a problem with an automated tool with no human oversight on the result it gives.  \n/r/ModSupport/comments/1iie3q9/issue_resolved_subreddit_banned_for_being/",
    "subreddit": "DataHoarder",
    "upvote_ratio": 0.96,
    "subreddit_type": "public",
    "ups": 3841,
    "downs": 0,
    "created_utc": 1738763820.0,
    "media": null,
    "is_video": false,
    "num_comments": 275,
    "num_reports": null,
    "over_18": false,
    "category": "value-add",
    "category_confidence": 0.6,
    "category_rationale": "Discussion of purge events and backup guidance for preservation.",
    "composite_score": 5.525011901558283
  },
  {
    "score": 3839,
    "title": "Please donate to Internet Archive!",
    "selftext": "Please for gods sake, to everyone who loves preserving things, donate to them if you can!\n\narchive.org/donate\n\nIA is getting dozens of DDOS attacks, hacks and lawsuits, to that they maybe need to shut down in the near future and it would be a shame when this holy moly grail of beautyful preservation history will be lost forever. \n\nWe need this preservation, so that we can experience this amout of beautyful little things, that got preserved for the future of humankind and can always be revisited/experienced.\n\n\nThank you.",
    "subreddit": "DataHoarder",
    "upvote_ratio": 0.97,
    "subreddit_type": "public",
    "ups": 3839,
    "downs": 0,
    "created_utc": 1728584360.0,
    "media": null,
    "is_video": false,
    "num_comments": 304,
    "num_reports": null,
    "over_18": false,
    "category": "other",
    "category_confidence": 0.6,
    "category_rationale": "Call to donate to Internet Archive; informational solicitation.",
    "composite_score": 5.553981144040924
  },
  {
    "score": 3719,
    "title": "Does anyone else feel like the internet is slowly being deleted?",
    "selftext": "Okay this might sound insane but the internet feels smaller?\n\nLike every week i go to rewatch something and it’s just gone. not archived, not mirrored, not torrented, nothing.\n\nCompanies keep editing old stuff, deleting scenes, removing episodes, rewriting history like we won’t notice. and everyone’s just chill about it?\n\nI swear one day we’ll wake up and half of the internet is just a 404 page.\n\nIs this just me going full tinfoil hat or is something seriously off?  \n",
    "subreddit": "DataHoarder",
    "upvote_ratio": 0.97,
    "subreddit_type": "public",
    "ups": 3719,
    "downs": 0,
    "created_utc": 1765784470.0,
    "media": null,
    "is_video": false,
    "num_comments": 436,
    "num_reports": null,
    "over_18": false,
    "category": "rant",
    "category_confidence": 0.4,
    "category_rationale": "Emotional concern about the internet being erased or degraded.",
    "composite_score": 5.618283658367108
  },
  {
    "score": 3709,
    "title": "Since the government just requested that republicans scrub January 6, 2021 from the Internet, post your favorite videos for us to back up",
    "selftext": "Links are good, torrents are good! Highest priority should be videos from government-controlled sources and archives.\n\n**Trump Instructs Republicans to 'Erase' January 6 Riots From History, Congressman Says**\n\nhttps://www.latintimes.com/trump-instructs-republicans-erase-january-6-riots-history-congressman-says-583747\n\nedit: The above article apparently refers to a plaque commemorating the Jan 6 riots. So there’s no evidence that Trump ordered the erasure of Jan 6, but I could easily see him ordering that, so I guess take this as a training drill to preserve this evidence!\n\nR/DataHoarder on January 31, 2021 created a compilation of 1 TB of videos into a torrent magnet link, you can read about it here: https://www.reddit.com/r/DataHoarder/s/TzzSdLhbXI\n\nEdit 2:\n\nNon American Redditors, please help! Make sure to seed this into the end of time so we Americans can never forget!\n\nHere’s a link to the magnet link for the compiled torrent:\n\nmagnet:?xt=urn:btih:c8fc9979cc35f7062cd8715aaaff4da475d2fadc",
    "subreddit": "DataHoarder",
    "upvote_ratio": 0.94,
    "subreddit_type": "public",
    "ups": 3709,
    "downs": 0,
    "created_utc": 1748035147.0,
    "media": null,
    "is_video": false,
    "num_comments": 173,
    "num_reports": null,
    "over_18": false,
    "category": "other",
    "category_confidence": 0.6,
    "category_rationale": "Call to back up videos and share sources; preservation-focused but not instructional.",
    "composite_score": 5.394648533756346
  },
  {
    "score": 3612,
    "title": "The Internet Archive needs to genuinely discuss moving to a country that's less hostile towards it's existence.",
    "selftext": "The United States, current 'politics' aside, was never hospitable for free information. Their copyright system takes a lifetime for fair use to kick in, and they always side with corporations in court.\n\n\nThe IA needs to both acknowledge these and move house. The only way I think they could be worse off for their purposes is if they were somewhere like Japan. \n\nSweden has historically been a good choice for Freedom of Information. ",
    "subreddit": "DataHoarder",
    "upvote_ratio": 0.97,
    "subreddit_type": "public",
    "ups": 3612,
    "downs": 0,
    "created_utc": 1745331434.0,
    "media": null,
    "is_video": false,
    "num_comments": 163,
    "num_reports": null,
    "over_18": false,
    "category": "value-add",
    "category_confidence": 0.62,
    "category_rationale": "Presents a central view about relocating IA for better information freedom with supporting context.",
    "composite_score": 5.3927898855918714
  },
  {
    "score": 3238,
    "title": "Epstein Files - For Real",
    "selftext": "A few hours ago there was a post about processing the Epstein files into something more readable, collated and what not. Seemed to be a cash grab. \n\nI have now processed 20% of the files, in 4 hours, and uploaded to GitHub, including transcriptions, a statically built and searchable site, the code that processes them (using a self hosted installation of llama 4 maverick VLM on a very big server. I’ll push the latest updates every now and then as more documents are transcribed and then I’ll try and get some dedupe. \n\nIt processes and tries to restore documents into a full document from the mixed pages - some have errored, but will capture them and come back to fix. \n\nI haven’t included the original files - save space on GitHub - but all json transcriptions are readily available. \n\nIf anyone wants to have a play, poke around or optimise - feel free\n\nTotal cost, $0. Total hosting cost, $0. \n\nNot here to make a buck, just hoping to collate and sort through all these files in an efficient way for everyone. \n\nhttps://epstein-docs.github.io\n\nhttps://github.com/epstein-docs/epstein-docs.github.io\n\nmagnet:?xt=urn:btih:5158ebcbbfffe6b4c8ce6bd58879ada33c86edae&amp;dn=epstein-docs.github.io&amp;tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce",
    "subreddit": "DataHoarder",
    "upvote_ratio": 0.98,
    "subreddit_type": "public",
    "ups": 3238,
    "downs": 0,
    "created_utc": 1759736235.0,
    "media": null,
    "is_video": false,
    "num_comments": 333,
    "num_reports": null,
    "over_18": false,
    "category": "playbook",
    "category_confidence": 0.78,
    "category_rationale": "Outlines a reproducible workflow to process, host, dedupe, and publish Epstein-related documents.",
    "composite_score": 5.507284181415959
  }
]