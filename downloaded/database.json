[
  {
    "score": 300,
    "title": "How does a database find one row so fast inside GBs of data?",
    "selftext": "Ohkk this has been in my head for days lol like when ppl say ‚Äúthe database has millions of rows‚Äù or ‚Äúa few GB of data‚Äù then how does it still find one row so fast when we do smtg like \n\nExample : \"SELECT * FROM users WHERE id = 123;\"\n\nImean like is the DB really scanning all rows super fast or does it jump straight to the right place somehow? How do indexes actually work in simple terms? Are they like a sorted list, a tree, a hash table or smtg else? On disk, is the data just a big file with rows one after another or is it split into pages/blocks and the DB jumps btwn them? And what changes when there are too many indexes and ppl say ‚Äúwrites get slow‚Äù?? \n ",
    "subreddit": "Database",
    "upvote_ratio": 0.95,
    "subreddit_type": "public",
    "ups": 300,
    "downs": 0,
    "created_utc": 1765091813.0,
    "media": null,
    "is_video": false,
    "num_comments": 92,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 4.175307969870811
  },
  {
    "score": 237,
    "title": "Is there any legitimate technical reason to introduce OracleDB to a company?",
    "selftext": "There are tons of relational database services out there, but only Oracle has a history of suing and overcharging its customers.\n\nI understand why a company would stick with Oracle if they‚Äôre already using it, but what I don‚Äôt get is why anyone would *adopt* it now. How does Oracle keep getting new customers with such a hostile reputation?\n\nMy assumption is that new customers follow the old saying, ‚ÄúNobody ever got fired for buying IBM,‚Äù only now it‚Äôs ‚ÄúOracle.‚Äù\n\nThat is to say, they go with a reputable firm, so no one blames them if the system fails. After all, they can claim \"Oracle is the best and oldest. If they failed, this was unavoidable and not due to my own technical incompetence.\"\n\nIt may also be that a company adopts Oracle because their CTO used it in their previous work and is too unwilling to learn a new stack.\n\nI'm truly wondering, though, if there are legitimate technical advantages it offers that makes it better than other RDBMS.",
    "subreddit": "Database",
    "upvote_ratio": 0.94,
    "subreddit_type": "public",
    "ups": 237,
    "downs": 0,
    "created_utc": 1761935606.0,
    "media": null,
    "is_video": false,
    "num_comments": 234,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 4.26711088819238
  },
  {
    "score": 213,
    "title": "Does this dataset warrant MongoDB",
    "selftext": "So i am on a  journey to learn new languages and tools and i am building a small side project with everything that i learn. I want to try build a system with mongodb and i want to know would this example be better for a traditional relational db or mongodb. \n\nIts just a simple system where i have games on a site, and users can search and filter through the games. As well as track whether they have completed the game or not.",
    "subreddit": "Database",
    "upvote_ratio": 0.94,
    "subreddit_type": "public",
    "ups": 213,
    "downs": 0,
    "created_utc": 1755423004.0,
    "media": null,
    "is_video": false,
    "num_comments": 82,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 3.994952819537228
  },
  {
    "score": 191,
    "title": "Hypothetically Someone Dropped the Database what should I do",
    "selftext": "we use MSSQL 2019\n\nand yea so hypothetically my manager dropped the database which in turn deleted all the stored procedures I needed for an application development, and hypothetically the development database is never backed up, cause hypothetically my manager is brain dead, is there any way I can restore all the SPs?\n\nEDIT: The database was dropped on a weekend while I'm sipping morning coffee, and yes its only the DevDB not the production so as the only developer in the company I'm the only one affected.\n\nEDIT2:I asked the Manager about the script used for the drop and its detached, and it'll delete the MDF and logs, copy the upper environment's MDF and logs and rename it as the devs, the recycle bin doesnt have the mdf and logs, full recovery is on simple mode\n\nLast Edit: I fixed the problem?? I recreated my sprocs, added them to git using the database project on visual studio, and added a backup procedure on my development environment. good thing I have my sprocs stored at the little corner of my head.\n\nfor those saying I should've created the back up as soon as possible, time constraints wouldnt let me. the President which dont know a thing about the technicalities of such things want something to be presented within a month of my employment. so all other procedures are thrown at the back lines of my job list, and the supposed problem...erm Manager didnt give me an access to the server and only gave it to me when the database was dropped and I only have some read and write access on windows auth.\n\nThanks to ya'all",
    "subreddit": "Database",
    "upvote_ratio": 0.95,
    "subreddit_type": "public",
    "ups": 191,
    "downs": 0,
    "created_utc": 1765353321.0,
    "media": null,
    "is_video": false,
    "num_comments": 146,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 4.079459896077637
  },
  {
    "score": 177,
    "title": "Databases are overrated. Just use Files!",
    "selftext": "I had a conversation on LinkedIn with a startup founder, and it opened my eyes. Why all the dependencies? The world can be so simple without databases and their complicated architecture. ùêâùêÆùê¨ùê≠ ùêÆùê¨ùêû ùêüùê¢ùê•ùêûùê¨! Alleluja! üôè\n\n[https://fooba.link/no-databases](https://fooba.link/no-databases) \n\nYes, just dump data into JSON format and save it to disk. Load it on demand and iterate over the rows. To make it more performant, cache the data in RAM. And yes, you need a unique key lookup table. And maybe a hash table for indexes. And then maybe a proper storage layer. And a query planner would be good. And ... And ...  \nWait a minute! Are we developing a database here? ü§® AAAAAhhhh....!!! üò≠",
    "subreddit": "Database",
    "upvote_ratio": 0.95,
    "subreddit_type": "public",
    "ups": 177,
    "downs": 0,
    "created_utc": 1710414655.0,
    "media": null,
    "is_video": false,
    "num_comments": 112,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 3.9894592240506035
  },
  {
    "score": 163,
    "title": "Why is it considered a cardinal sin to store a file's raw content along the metadata in SQL Database?",
    "selftext": "Short background, I currently am working on a small project at work that involves a Postgres Database, .NET Backend as well as a bunch of files users can run CRUD operations on. Its a pretty low frequency app that never is used by more than 3 people at the same time and the files we are talking are in the 1 - 10 mb range. \n\nOne thing most developers (who mostly write Backend code in C#, python, java, ... and not SQL) seem to believe that it is a cardinal sin to store the contents of the files directly inside the database, yet seem happy to store all the metadata like filename, last access, owners, ... in there. In my opinion this causes a number of issues - full backups of the system become more complicated, there is no easy mechanism to guarantee atomicity on operations like there is on a db with transactions (for example deleting a file might delete the record form the table, but not the actual file on the filesystem because some other process has a lock on it), having files both on the disk and the db limits how much you can normalize (for example the filename and location need to be stored redundantly ... also in theory a file could exist in the db but not on the filesystem anymore or the other way around).\n\nI get that you might cause some overhead from having to go through another layer (the DB) to stream the content of your file, but I feel like unless your application has a huge number of concurrent users¬¥streaming giant files, any reasonable modern server should handle this with ease.\n\nCurious to hear the opinion of other people from the DB side or what I'm overlooking.",
    "subreddit": "Database",
    "upvote_ratio": 0.93,
    "subreddit_type": "public",
    "ups": 163,
    "downs": 0,
    "created_utc": 1765970977.0,
    "media": null,
    "is_video": false,
    "num_comments": 224,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 4.088435107103379
  },
  {
    "score": 134,
    "title": "What are the reasons *not* to migrate from MySQL to PostgreSQL?",
    "selftext": "With the recent news about[¬†mass layoffs of the MySQL staff](https://www.theregister.com/2025/09/11/oracle_slammed_for_mysql_job/) at Oracle, no [git commits](https://github.com/mysql/mysql-server/commits/trunk/)¬†in real time on GitHub since long time ago and with the new releases clear signs that [Oracle isn't adding new features](https://www.percona.com/blog/is-oracle-finally-killing-mysql/) seems a lot of architects and DBAs are now scrambling for migration plans (if still on MySQL, many moved to MariaDB years ago of course).\n\nFor those running their own custom app with full freedom to rearchitect the stack, or using the database via an ORM that allows them to easily switch the database, many seem to be planning to migrate to PostgreSQL, which is mature and has a large and real open source community and wide ecosystem support.\n\nWhat would the reasons be to **not** migrate from MySQL to PostgreSQL? Is autovacuuming in PostgreSQL still slow and logical replication tricky? Does the famous [Uber blog post about PostgreSQL performance isues](https://www.uber.com/en-CA/blog/postgres-to-mysql-migration/) still hold? What is the most popular multi-master replication solution in PostgreSQL (similar to Galera)?",
    "subreddit": "Database",
    "upvote_ratio": 0.95,
    "subreddit_type": "public",
    "ups": 134,
    "downs": 0,
    "created_utc": 1760736673.0,
    "media": null,
    "is_video": false,
    "num_comments": 79,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 3.794378761990978
  },
  {
    "score": 112,
    "title": "Stored Procedures vs No Stored Procedures",
    "selftext": "\nRecently, I posted about my stored procedures getting deleted because the development database was dropped.\n\nI saw some conflicting opinions saying that using stored procedures in the codebase is bad practice, while others are perfectly fine with it.\n\nTo give some background: I‚Äôve been a developer for about 1.5 years, and 4 months of that was as a backend developer at an insurance company. That‚Äôs where I learned about stored procedures, and I honestly like them, the sense of control they give and the way they allow some logic to be separated from the application code.\n\nNow for the question: why is it better to use stored procedures, why is it not, and under what conditions should you use or avoid them?\n\nMy current application is quite data intensive, so I opted to use stored procedures. I‚Äôm currently working in .NET, using an ADO.NET wrapper that I chain through repository classes.",
    "subreddit": "Database",
    "upvote_ratio": 0.93,
    "subreddit_type": "public",
    "ups": 112,
    "downs": 0,
    "created_utc": 1766101847.0,
    "media": null,
    "is_video": false,
    "num_comments": 218,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 3.9208005009034785
  },
  {
    "score": 105,
    "title": "Transitioning a company from Excel spreadsheets to a database for data storage",
    "selftext": "I recently joined a small investment firm that has around 30 employees and is about 3 years old. Analysts currently collect historical data in Excel spreadsheets related to companies we own or are evaluating, so there isn‚Äôt a centralized place where data lives and there‚Äôs no real process for validating it. I‚Äôm the first programmer or data-focused hire they‚Äôve brought on. Everyone is on Windows.\n\nThe amount of data we‚Äôre dealing with isn‚Äôt huge, and performance or access speed isn‚Äôt a major concern. Given that, what databases should a company like this be looking at for storing data?",
    "subreddit": "Database",
    "upvote_ratio": 0.96,
    "subreddit_type": "public",
    "ups": 105,
    "downs": 0,
    "created_utc": 1766548748.0,
    "media": null,
    "is_video": false,
    "num_comments": 67,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 3.6615603216178885
  },
  {
    "score": 104,
    "title": "I‚Äôve finally launched DB Pro: a modern desktop database GUI I‚Äôve been building for 3 months",
    "selftext": "Hey everyone! After three months of designing, building, rewriting, and polishing, I‚Äôve just launched **DB Pro**, a modern desktop app for working with databases.\n\nIt‚Äôs built to be fast, clean, and actually enjoyable to use with features like:\n\n  \n‚Ä¢ a visual schema viewer  \n‚Ä¢ inline data editing  \n‚Ä¢ raw SQL editor  \n‚Ä¢ activity logs  \n‚Ä¢ custom table tagging  \n‚Ä¢ multiple tabs/windows  \n‚Ä¢ and more on the way\n\nYou can download it free for macOS here: [**https://dbpro.app/download**]()  \n  \n(Windows + Linux versions are coming soon.)\n\nIf you‚Äôre curious about the build process, I‚Äôm documenting everything in a devlog series. Here‚Äôs the latest episode:  \n[**https://www.youtube.com/watch?v=-T4GcJuV1rM**](https://www.youtube.com/watch?v=-T4GcJuV1rM)\n\nI‚Äôd love any feedback. UI, UX, features, anything.  \n  \nCheers!",
    "subreddit": "Database",
    "upvote_ratio": 0.89,
    "subreddit_type": "public",
    "ups": 104,
    "downs": 0,
    "created_utc": 1764252081.0,
    "media": {
      "reddit_video": {
        "bitrate_kbps": 5000,
        "fallback_url": "https://v.redd.it/gunmrs8y2t3g1/CMAF_1080.mp4?source=fallback",
        "has_audio": true,
        "height": 1080,
        "width": 1920,
        "scrubber_media_url": "https://v.redd.it/gunmrs8y2t3g1/CMAF_96.mp4",
        "dash_url": "https://v.redd.it/gunmrs8y2t3g1/DASHPlaylist.mpd?a=1770354067%2COTg1ZmViMjcyNzcyZWUxOWM0ODljMWEyOTZlNGJjMmNlYjg4NTIzNTY5MzJlZjc0NjFiZGQ2OGNkODdhZmMwOQ%3D%3D&amp;v=1&amp;f=sd",
        "duration": 62,
        "hls_url": "https://v.redd.it/gunmrs8y2t3g1/HLSPlaylist.m3u8?a=1770354067%2CMWI2MjI1ZmFjY2RlNjM4MmIxNjI2Nzc3YWZhYzMwNzk5ZmMxNDNkNGUxMmFmNWMwMGFiOWRlOWE5ZGExMTkxOA%3D%3D&amp;v=1&amp;f=sd",
        "is_gif": false,
        "transcoding_status": "completed"
      }
    },
    "is_video": true,
    "num_comments": 38,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 3.484221602583188
  },
  {
    "score": 85,
    "title": "Just released a free, browser-based DB UI with AI assistant",
    "selftext": "Hi all, pleasure to join this community!\n\nAs a fullstack engineer and I've long been dissatisfied with the database UIs out there. So I set out to develop the most fun to use, user-friendly UI for databases that I can come up with.\n\nAfter 2 years of work, here is¬†[smartquery.dev](http://smartquery.dev/), a browser-based UI for Postgres, MySQL, and SQLite. And of course, with a strong focus on AI: Next to inline completions you get a chat that knows the schema definitions of your DB and can generate very accurate SQL.\n\nIt's free to use and I would be super grateful for any feedback.\n\nUpdate: Source code now published at [https://github.com/simon-mathewson/smartquery](https://github.com/simon-mathewson/smartquery)",
    "subreddit": "Database",
    "upvote_ratio": 0.89,
    "subreddit_type": "public",
    "ups": 85,
    "downs": 0,
    "created_utc": 1757511555.0,
    "media": null,
    "is_video": false,
    "num_comments": 40,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 3.408390379603435
  },
  {
    "score": 71,
    "title": "I built a billion scale vector database from scratch that handles bigger than RAM workloads",
    "selftext": "[](https://www.reddit.com/r/rust/)I've been working on SatoriDB, an embedded vector database written in Rust. The focus was on handling billion-scale datasets without needing to hold everything in memory.\n\n[](https://preview.redd.it/i-built-a-billion-scale-vector-database-from-scratch-that-v0-sraqllttav8g1.png?width=9545&amp;format=png&amp;auto=webp&amp;s=42aaaa82234b9a66cf650fa2b63cf314ae59fa17)\n\nit has:\n\n* 95%+ recall on BigANN-1B benchmark (1 billion vectors, 500gb on disk)\n* Handles bigger than RAM workloads efficiently\n* Runs entirely in-process, no external services needed\n\nhttps://preview.redd.it/awyki45t05bg1.png?width=1536&amp;format=png&amp;auto=webp&amp;s=e6a683d8a3a97893888e747441f5c67b685f4f48\n\n  \n  \nHow it's fast:\n\nThe architecture is two tier search. A small \"hot\" HNSW index over quantized cluster centroids lives in RAM and routes queries to \"cold\" vector data on disk. This means we only scan the relevant clusters instead of the entire dataset.\n\nI wrote my own HNSW implementation (the existing crate was slow and distance calculations were blowing up in profiling). Centroids are scalar-quantized (f32 ‚Üí u8) so the routing index fits in RAM even at 500k+ clusters.\n\nStorage layer:\n\nThe storage engine (Walrus) is custom-built. On Linux it uses io\\_uring for batched I/O. Each cluster gets its own topic, vectors are append-only. RocksDB handles point lookups (fetch-by-id, duplicate detection with bloom filters).\n\nQuery executors are CPU-pinned with a shared-nothing architecture (similar to how ScyllaDB and Redpanda do it). Each worker has its own io\\_uring ring, LRU cache, and pre-allocated heap. No cross-core synchronization on the query path, the vector distance perf critical parts are optimized with handrolled SIMD implementation\n\nI kept the API dead simple for now:\n\n    let db = SatoriDb::open(\"my_app\")?;\n    \n    db.insert(1, vec![0.1, 0.2, 0.3])?;\n    let results = db.query(vec![0.1, 0.2, 0.3], 10)?;\n\nLinux only (requires io\\_uring, kernel 5.8+)\n\nCode: [https://github.com/nubskr/satoridb](https://github.com/nubskr/satoridb)\n\nwould love to hear your thoughts on it :)",
    "subreddit": "Database",
    "upvote_ratio": 0.89,
    "subreddit_type": "public",
    "ups": 71,
    "downs": 0,
    "created_utc": 1767447489.0,
    "media": null,
    "is_video": false,
    "num_comments": 5,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 2.9139081216230904
  },
  {
    "score": 64,
    "title": "Explore and learn the basics of SQL via typing practice",
    "selftext": "Hello üëã\n\nI'm one of the software engineers on TypeQuicker. \n\nMost of my previous jobs involved working with some SQL database (usually Postgres or MySQL) and throughout the day, I would frequently need to query some data and writing queries without having to look up certain uncommon keywords became a cause of friction for me.\n\nIn the past I used Anki cards to study various language keywords - but I find this makes it even more engaging and fun!\n\nHelpful for discovery, learning and re-enforcing your SQL skill (or any programming language or tool for that matter)",
    "subreddit": "Database",
    "upvote_ratio": 0.98,
    "subreddit_type": "public",
    "ups": 64,
    "downs": 0,
    "created_utc": 1757197432.0,
    "media": {
      "reddit_video": {
        "bitrate_kbps": 5000,
        "fallback_url": "https://v.redd.it/lp2yt8x2emnf1/DASH_1080.mp4?source=fallback",
        "has_audio": false,
        "height": 1080,
        "width": 1720,
        "scrubber_media_url": "https://v.redd.it/lp2yt8x2emnf1/DASH_96.mp4",
        "dash_url": "https://v.redd.it/lp2yt8x2emnf1/DASHPlaylist.mpd?a=1770354067%2CZWU4OTlhMjI1YTIwZGFkYzE3Y2EyMzlmOTY0OTgwZGY5ZTdhMDg3ZmVjZmNlZjA1NjY4NDMxMmNjYTE1YWM0Zg%3D%3D&amp;v=1&amp;f=sd",
        "duration": 27,
        "hls_url": "https://v.redd.it/lp2yt8x2emnf1/HLSPlaylist.m3u8?a=1770354067%2CNzkwNjMwYjgyMzBjMWVhNmYzYzBlMjg5MzQwY2JmM2Y2NGZlZTAxN2JhZWFjZDc4NjI0M2E5OTNjYTg4YTM2Mg%3D%3D&amp;v=1&amp;f=sd",
        "is_gif": false,
        "transcoding_status": "completed"
      }
    },
    "is_video": true,
    "num_comments": 16,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 3.163137817331992
  },
  {
    "score": 57,
    "title": "Database Schema Review",
    "selftext": "I'm planning on making a Fitness Tracking app with Users for my project and I wanted it to be a fully featured system. I've based it on a fitness program I applied for. This [spreadsheet](https://imgur.com/a/DihWyqp) (spreadsheet not mine) is what I based the schema on.\n\nI'm having trouble whether I should just put all the daily metric tracker in one table (hence DailyMetrics table) and omit every individual trackers to remove redundancy or keep it to have a more verbose information for each trackers made by the student.\n\nAlso, is my idea of habit tracking tables also correct?\n\nIf you'd like to see more of the diagram, you check it [here](https://dbdocs.io/agcabezas/Project-Lung-aw)\n\nI'd appreciate every insight and criticism about my approach!",
    "subreddit": "Database",
    "upvote_ratio": 0.94,
    "subreddit_type": "public",
    "ups": 57,
    "downs": 0,
    "created_utc": 1753061398.0,
    "media": null,
    "is_video": false,
    "num_comments": 26,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 3.184109875642431
  },
  {
    "score": 56,
    "title": "What advantages do NoSQL databases have over relational ones?",
    "selftext": "Every article I look at claims horizontal scaling to be the biggest advantage in favour of NoSQL databases, but that's not even true since there are multiple solutions to horizontal scaling of relational databases if I am not mistaken.\n\nSo what advantages do they actually have? Does it depend on the specific implementation of NoSQL database?\n\nLike graph database being able to handle relationships better, wide-column being better when you mainly work with specific columns within a row  etc.... \n\nOr is it the fact that they handle unstructured data better?\n\nBut isn't it possible to optimise relational database for the same purposes?",
    "subreddit": "Database",
    "upvote_ratio": 0.92,
    "subreddit_type": "public",
    "ups": 56,
    "downs": 0,
    "created_utc": 1713981629.0,
    "media": null,
    "is_video": false,
    "num_comments": 63,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 3.348964842664435
  },
  {
    "score": 53,
    "title": "There seems to be an anti-RDBMS cult out there",
    "selftext": "In the early 2000's many dot-com startups realized that DBA's didn't understand a start-up's needs, and thus \"got in the way\" of \"moving fast and breaking things\".  Therefore, they started avoiding the RDBMS and/or DBA's by reinventing many of their features in app code. The RDBMS started being used a mere \"dumb storage\" with other functions being moved to app code.\n\nThere was a culture-misfit problem, I don't dispute that. But it's not a technology problem, but a management and training problem. DBA's needed to be educated on start-up practices and needs.\n\nRDBMS skills thus atrophied, and such shops grew used to doing everything through apps and app interfaces (web-services &amp; microservices). I'm open to new ideas, but I don't see their app-centric approach to data as an improvement. RDBMS do most data-oriented things BETTER because they are built with data in mind. It's also just wasted busy-work to JSON-itize everything, being more total code, more layers, and more things to manage and break.\n\nFor example, a stored procedure is often the simplest solution, but instead they make it a JSON web service. KISS usually dictates a stored procedure. And the database can also often be used as a communication conduit between many \"sub-apps\" (a *non*-monolith) such that a JSON interface layer is wasted busy work. YAGNI dictates \"skippit!\".\n\nMany pro-appers will say **\"microservices can do X but RDBMS can't.\"** I prove them otherwise, and they say, \"well, um, but it's harder in a RDBMS\". It usually turns out that it's \"harder\" because they are app coders, not DBA's or don't want to hire DBA's because they are used to appers and app-centric thinking. Top managers in many startups are coders, not DBA's, and so view every problem through an app-coder's perspective. \n\n(Maybe for giant companies or non-critical \"web-scale\", RDBMS are not a good fit, but my observations focus on non-giant orgs. The app-centric approach may be better for \"web scale\", I don't know and can't say, but most readers probably don't work for Netflix-sized. The JSON fans also claim JSON-wrapping makes it easier to swap DB brands, but I'm skeptical; plus it's rare enough to not spend layers up front on it: YAGNI. When they overhaul a DB, a shop usually also overhauls the apps, making 1-to-1 swapping moot.) \\[edited.\\]\n\nAnd also many current IT managers came up as a coder, and thus think app-first instead of database first, and thus don't understand how to leverage a database. At the very least, I sense a **culture conflict:** the Appers vs. DBers. Sometimes they imply DB-ing is obsolete and that I'm just a nostalgic geezer. However, I don't see the JSON-way a net simplifier. The Appers can't show clear-cut simplification. (The challenge still stands, bring it on!) The youngbies are just not used to fully leveraging the DB.",
    "subreddit": "Database",
    "upvote_ratio": 0.85,
    "subreddit_type": "public",
    "ups": 53,
    "downs": 0,
    "created_utc": 1680462276.0,
    "media": null,
    "is_video": false,
    "num_comments": 71,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 3.298560008038603
  },
  {
    "score": 44,
    "title": "I built Advent of SQL - An Advent of Code style daily SQL challenge with a Christmas mystery story",
    "selftext": "Hey all,\n\nI‚Äôve been working on a fun December side project and thought this community might appreciate it.\n\nIt‚Äôs called **Advent of SQL.** You get a daily set of SQL puzzles (similar vibe to Advent of Code, but entirely database-focused).  \n  \nEach day unlocks a new challenge involving things like:\n\n* JOINs\n* GROUP BY + HAVING\n* window functions\n* string manipulation\n* subqueries\n* real-world-ish log parsing\n* and some quirky Christmas-world datasets\n\nThere‚Äôs also a light mystery narrative running through the puzzles (a missing reindeer, magical elves, malfunctioning toy machines, etc.), but the SQL is very much the main focus.\n\nIf you fancy doing a puzzle a day, here‚Äôs the link:\n\nüëâ [**https://www.dbpro.app/advent-of-sql**](https://www.dbpro.app/advent-of-sql)\n\nIt‚Äôs free and I mostly made this for fun alongside my DB desktop app. Oh, and you can solve the puzzles right in your browser. I used an embedded SQLite. Pretty cool!\n\n(Yes, it's 11 days late, but that means you guys get 11 puzzles to start with!)",
    "subreddit": "Database",
    "upvote_ratio": 0.91,
    "subreddit_type": "public",
    "ups": 44,
    "downs": 0,
    "created_utc": 1765463810.0,
    "media": null,
    "is_video": false,
    "num_comments": 0,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 2.3357125137753436
  },
  {
    "score": 40,
    "title": "The most space efficient database?",
    "selftext": "I am a data hoarder. I have several databases at home, including a PostgreSQL database that takes up over 20TB of space. I store everything there‚Äîfrom web pages I scrape (complete HTML files, organized data, and scraping logs) to sensor data and data exported from Prometheus or small files (i know i shouldn't). You can definitely call me a fan of this database because, despite its size, it still runs incredibly fast on slow HDDs (Seagate Basic).\n\nFor fun, I put most of the same data into MongoDB and, to my surprise, 19,270 GB of data occupies only 3,447 GB there. I measure this by invoking `db.stats(1024*1024*1024)` and then comparing by looking at `dataSize` and `totalSize`. Access to most of the data is managed through the value stored in PostgreSQL.\n\nNow, my question is, is there any database that will provide me with fast access to data on a hard disk while offering better compression? I am happy to test your suggestions! As it's home lab environment, i would like to avoid paid solutions.",
    "subreddit": "Database",
    "upvote_ratio": 0.98,
    "subreddit_type": "public",
    "ups": 40,
    "downs": 0,
    "created_utc": 1716144469.0,
    "media": null,
    "is_video": false,
    "num_comments": 40,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 3.154175785079603
  },
  {
    "score": 38,
    "title": "What are the reasons *not* to migrate from MySQL to MariaDB?",
    "selftext": "When Oracle originally acquired MySQL back in 2008, the European Commission launched a monopoly investigation and was initially going to block the deal as Oracle most likely wanted MySQL only to kill its competition. However, the deal was allowed. Most users understood what Oracle's ultimate motives are, and the original creators of MySQL forked it, and MariaDB was born.  \n  \nMany moved to MariaDB years ago, but not all. Although Oracle stopped releasing [git commits](https://github.com/mysql/mysql-server/commits/trunk/) in real time on GitHub long time ago, they kept releasing new MySQL versions for many years, and many MySQL users happily continued using it. [Last year there started to be more signs that Oracle is closer to actually killing MySQL](https://www.percona.com/blog/is-oracle-finally-killing-mysql/), and now this fall they announced[ mass layoffs of the MySQL staff](https://www.theregister.com/2025/09/11/oracle_slammed_for_mysql_job/), which seems to be the final nail in the coffin.\n\nWhat are people here still using MySQL planning to do now? What prevented you from migrating to MariaDB years ago? Have those obstacles been solved by now? Missing features? Missing ecosystem support? Lack of documentation?\n\nThere isn't that much public stats around, but for example [WordPress stats show that 50%+ are running MariaDB](https://wordpress.org/about/stats/#mysql_version). Did in fact the majority already switch to MariaDB for other apps too? As MySQL was so hugely popular in web development back in the days, one would think that this issue affects a lot of devs now and there would be a lot of people in need of sharing experiences, challenges and how they overcome them.",
    "subreddit": "Database",
    "upvote_ratio": 0.88,
    "subreddit_type": "public",
    "ups": 38,
    "downs": 0,
    "created_utc": 1760127684.0,
    "media": null,
    "is_video": false,
    "num_comments": 49,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 3.1005496091945086
  },
  {
    "score": 35,
    "title": "PostgreSQL 18 Released ‚Äî pgbench Results Show It‚Äôs the Fastest Yet",
    "selftext": "I just published a benchmark comparison across PG versions 12‚Äì18 using pgbench mix tests: \n\n[https://pgbench.github.io/mix/](https://pgbench.github.io/mix/)\n\nPG18 leads in every metric:\n\n* **3,057 TPS** ‚Äî highest throughput\n* **5.232 ms latency** ‚Äî lowest response time\n* **183,431 transactions** ‚Äî most processed\n\nThis is synthetic, but it‚Äôs a strong signal for transactional workloads. Would love feedback from anyone testing PG18 in production‚Äîany surprises or regressions?",
    "subreddit": "Database",
    "upvote_ratio": 0.94,
    "subreddit_type": "public",
    "ups": 35,
    "downs": 0,
    "created_utc": 1758974510.0,
    "media": null,
    "is_video": false,
    "num_comments": 2,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 2.4998631281271186
  },
  {
    "score": 39,
    "title": "We built a time series database with streaming capabilities that is optimized for sensor data",
    "selftext": "Hey all! I wanted to post about a custom time series database that we built that is optimized for sensor data.\n\nMy team and I are software engineers that come from various backgrounds in aerospace. We've seen several different ways that teams have tried to solve the problem of acquiring sensor data from hardware, storing the data in a database, and also streaming the data for usage by other consumers. We haven't been impressed by most of the solutions we've seen - they usually require an internal team of software engineers to frankenstein together data acquirers, a database, streaming services, and visualization software.\n\nWe ended up building Synnax (https://www.synnaxlabs.com), a custom time series database that also allows for live streaming of data. Synnax is horizontally scalable and fault-tolerant, and works by giving each sensor a bucket called a \"channel\" - equivalent to a column. The data for each channel is stored in its own file in the file system. Something that we've realized from building Synnax is that all databases are ultimately wrappers around a file system. We decided to manage reading and writing to files ourselves to keep the database more performant.\n\nSynnax also has the ability to open up a \"streamer\" on a specific channel, allowing for data to be read and acted on as soon as it is written. This means that automated hardware control scripts can be written that make control decisions as a value is getting written to another channel.\n\nReading data from and writing data into Synnax is done through our client libraries in C++, Python, or TypeScript. We wanted to make it easy to use Synnax for multiple applications, such as C++ for device drivers, Python for analysis tools, and TypeScript for making GUIs and visualizations.\n\nWe've also built some custom tools on top of Synnax for ease of adoption with hardware organizations. We have device drivers that can automatically connect to National Instruments hardware or PLCs through an OPC UA server. We've also built a visualization dashboard that can be used for plotting data (both live &amp; historical) and creating schematic diagram views which allows for hardware control.\n\nIf this sounds interesting to you, please download our software and check it out! You can download Synnax from our documentation site (https://docs.synnaxlabs.com), and our code is source-available, so you can also browse our GitHub (https://github.com/synnaxlabs/synnax). Usage of up to 50 channels is free, and if you are interested in using it for a larger project, please DM me for more info!\n\nIf you've worked with a database storing sensor data, I'd love it if you could answer some questions:\n\n1. What database do you use to store the data?How does the data end up getting piped into the database from the sensors?\n2. What's your biggest pain point or problem that you had / need to solve in building out this database?\n3. How do you manage streaming sensor data?\n",
    "subreddit": "Database",
    "upvote_ratio": 0.97,
    "subreddit_type": "public",
    "ups": 39,
    "downs": 0,
    "created_utc": 1723500549.0,
    "media": {
      "reddit_video": {
        "bitrate_kbps": 5000,
        "fallback_url": "https://v.redd.it/6cuejmny3bid1/DASH_1080.mp4?source=fallback",
        "has_audio": false,
        "height": 1080,
        "width": 1920,
        "scrubber_media_url": "https://v.redd.it/6cuejmny3bid1/DASH_96.mp4",
        "dash_url": "https://v.redd.it/6cuejmny3bid1/DASHPlaylist.mpd?a=1770354067%2COTZhMDhlZTZkODczMDBkNmQyZWJmZTMwMjVmMDU2MmI5YmRkZTBlMDdlZDY2MzM2ZDllZGNiMjc5ZjkxZjk0NA%3D%3D&amp;v=1&amp;f=sd",
        "duration": 5,
        "hls_url": "https://v.redd.it/6cuejmny3bid1/HLSPlaylist.m3u8?a=1770354067%2CN2I5MTBkMDIxYTBkNzZkZTc4OWI3NGU3M2UzZWI5M2QxYmMxODc1YzlhYTI1YjU1YWRlOTZmNzBhNzE3YjAxMQ%3D%3D&amp;v=1&amp;f=sd",
        "is_gif": false,
        "transcoding_status": "completed"
      }
    },
    "is_video": true,
    "num_comments": 9,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 2.8295599913279625
  },
  {
    "score": 33,
    "title": "I am managing a database with zero idea of how to do it.",
    "selftext": "Hi! \n\nI work in the energy sector, managing energy communities (citizen-driven associations that share renewable energy). We used to have a third party database which was way too expensive for what we wanted, and in the end we have created our own in mysql. \n\nThing is, although I have had to prepare all the tables and relationships between them (no easy task, let me tell you) I really have no fucking clue about \"good practices\", or how \"big\" is a big table or DB. \n\nAs the tables have hourly values, a single year for a user has  8760 values, currently with 3 columns, just for consumption data. This table was designed with a long format, using \"id\" for user querying (as I did not want to handle new column creation). This means that a 3 year table for 100 users is over 2.5M lines. Is this too much? Mind you - i see no way of changing this. Tables reach the hundreds of MBs **easily**. Again, I see no way of changing this other than having 100s of tables (which I believe is not the way). \n\nI have to query this data all the time for a lot of processes; could it be an issue at some point? The database will grow into the GBs with ease. It is just for consumption and generation information, but what the hell am  I supposed to do. \n\nDo you see a way around it, a problem to come...some glaring mistake?\n\nAny way, just some questions from someone who is in a bit over his head; cant be an expert in fucking everything lol, thanks!",
    "subreddit": "Database",
    "upvote_ratio": 0.89,
    "subreddit_type": "public",
    "ups": 33,
    "downs": 0,
    "created_utc": 1759914186.0,
    "media": null,
    "is_video": false,
    "num_comments": 34,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 2.971012939217393
  },
  {
    "score": 31,
    "title": "How do you decide between SQL or NoSQL in my case?",
    "selftext": "So I'm in charge of creating a tool which will eventually be part of a bigger system. The tool will be in charge of containing workers, managers, admins, appointments, a time-off system, teams, etc. The purpose of the tool is to create teams (containing managers and workers), create appointments, and have managers dispatch workers to appointments (eventually track their location as they make their way to the customer). \n\nI actually have most of the tool built but the backend (due to how other engineers forced me to do it) is in absolute shambles and I finally convinced them to use AWS. Currently I'm using MySQL, so I have to decide between RDS and Dynamo.   \n  \nHonestly, my main issue is that the tables in SQL change too frequently due to customer requirements be changed (like columns get added/changed too often) and SQL migrations are proving to be quite a pain (but it might be because I'm just unfamiliar with how to that). I have to update backend code, frontend, and another migration sql file to my collection (honestly a library at this point) of migration scripts xd. \n\nI haven't worked enough with NoSQL to know its problems. The only thing I'm worried about is if the current database is too relational for NoSQL.  ",
    "subreddit": "Database",
    "upvote_ratio": 0.87,
    "subreddit_type": "public",
    "ups": 31,
    "downs": 0,
    "created_utc": 1760727885.0,
    "media": null,
    "is_video": false,
    "num_comments": 130,
    "num_reports": null,
    "over_18": false,
    "category": "",
    "category_confidence": null,
    "category_rationale": "",
    "composite_score": 3.216285626147788
  }
]